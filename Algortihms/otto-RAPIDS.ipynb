{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"VER = 6\n\nimport pandas as pd, numpy as np\nfrom tqdm.notebook import tqdm\nimport os, sys, pickle, glob, gc\nfrom collections import Counter\nimport cudf, itertools\nprint('We will use RAPIDS version',cudf.__version__)","metadata":{"papermill":{"duration":3.036143,"end_time":"2022-11-10T16:03:24.014816","exception":false,"start_time":"2022-11-10T16:03:20.978673","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T02:57:12.636139Z","iopub.execute_input":"2023-05-23T02:57:12.637050Z","iopub.status.idle":"2023-05-23T02:57:16.252623Z","shell.execute_reply.started":"2023-05-23T02:57:12.636947Z","shell.execute_reply":"2023-05-23T02:57:16.251440Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"We will use RAPIDS version 21.10.01\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# CACHE FUNCTIONS\ndef read_file(f):\n    return cudf.DataFrame( data_cache[f] )\ndef read_file_to_cache(f):\n    df = pd.read_parquet(f)\n    df.ts = (df.ts/1000).astype('int32')\n    df['type'] = df['type'].map(type_labels).astype('int8')\n    return df\n\n# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\ndata_cache = {}\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\nfiles = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*')\nfor f in files: data_cache[f] = read_file_to_cache(f)\n\n# CHUNK PARAMETERS\nREAD_CT = 5\nCHUNK = int( np.ceil( len(files)/6 ))\nprint(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')","metadata":{"papermill":{"duration":0.063943,"end_time":"2022-11-10T16:03:24.091816","exception":false,"start_time":"2022-11-10T16:03:24.027873","status":"completed"},"tags":[],"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-05-23T02:57:16.254568Z","iopub.execute_input":"2023-05-23T02:57:16.255035Z","iopub.status.idle":"2023-05-23T02:58:47.921233Z","shell.execute_reply.started":"2023-05-23T02:57:16.254988Z","shell.execute_reply":"2023-05-23T02:58:47.919984Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"We will process 146 files, in groups of 5 and chunks of 25.\nCPU times: user 1min 1s, sys: 11 s, total: 1min 12s\nWall time: 1min 31s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntype_weight = {0:1, 1:5, 2:4}\n\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            \n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            \n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            \n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            \n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n            df['wgt'] = df.type_y.map(type_weight)\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            \n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        \n        print()\n        \n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    \n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n    \n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')","metadata":{"papermill":{"duration":566.561189,"end_time":"2022-11-10T16:12:50.666123","exception":false,"start_time":"2022-11-10T16:03:24.104934","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T02:58:47.924052Z","iopub.execute_input":"2023-05-23T02:58:47.924520Z","iopub.status.idle":"2023-05-23T03:02:13.280994Z","shell.execute_reply.started":"2023-05-23T02:58:47.924478Z","shell.execute_reply":"2023-05-23T03:02:13.279701Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 24 in groups of 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n  \"When using a sequence of booleans for `ascending`, \"\n","output_type":"stream"},{"name":"stdout","text":"0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 2\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 3\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 4\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \nCPU times: user 2min 12s, sys: 1min 10s, total: 3min 23s\nWall time: 3min 25s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 1\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            \n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            \n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            \n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n            \n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            \n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y', 'type_y'])\n            df['wgt'] = 1\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            \n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n\n        print()\n        \n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    \n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n    \n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":113.735315,"end_time":"2022-11-10T16:14:44.498182","exception":false,"start_time":"2022-11-10T16:12:50.762867","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T03:02:13.284570Z","iopub.execute_input":"2023-05-23T03:02:13.285024Z","iopub.status.idle":"2023-05-23T03:02:45.499724Z","shell.execute_reply.started":"2023-05-23T03:02:13.284978Z","shell.execute_reply":"2023-05-23T03:02:45.498397Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , ","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/cudf/core/frame.py:2600: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n  \"When using a sequence of booleans for `ascending`, \"\n","output_type":"stream"},{"name":"stdout","text":"10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \nCPU times: user 22.4 s, sys: 9.26 s, total: 31.7 s\nWall time: 32.2 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# COMPUTE IN PARTS FOR MEMORY MANGEMENT\nfor PART in range(DISK_PIECES):\n    print()\n    print('### DISK PART',PART+1)\n    \n    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n    # => OUTER CHUNKS\n    for j in range(6):\n        a = j*CHUNK\n        b = min( (j+1)*CHUNK, len(files) )\n        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n        \n        # => INNER CHUNKS\n        for k in range(a,b,READ_CT):\n            # READ FILE\n            df = [read_file(files[k])]\n            for i in range(1,READ_CT): \n                if k+i<b: df.append( read_file(files[k+i]) )\n            df = cudf.concat(df,ignore_index=True,axis=0)\n            df = df.sort_values(['session','ts'],ascending=[True,False])\n            \n            # USE TAIL OF SESSION\n            df = df.reset_index(drop=True)\n            df['n'] = df.groupby('session').cumcount()\n            df = df.loc[df.n<30].drop('n',axis=1)\n            \n            # CREATE PAIRS\n            df = df.merge(df,on='session')\n            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n            \n            # MEMORY MANAGEMENT COMPUTE IN PARTS\n            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n            \n            # ASSIGN WEIGHTS\n            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n            # 1659304800 : minimum timestamp\n            # 1662328791 : maximum timestamp\n            df = df[['aid_x','aid_y','wgt']]\n            df.wgt = df.wgt.astype('float32')\n            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n            \n            # COMBINE INNER CHUNKS\n            if k==a: tmp2 = df\n            else: tmp2 = tmp2.add(df, fill_value=0)\n            print(k,', ',end='')\n        print()\n        \n        # COMBINE OUTER CHUNKS\n        if a==0: tmp = tmp2\n        else: tmp = tmp.add(tmp2, fill_value=0)\n        del tmp2, df\n        gc.collect()\n\n    # CONVERT MATRIX TO DICTIONARY\n    tmp = tmp.reset_index()\n    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n    \n    # SAVE TOP 40\n    tmp = tmp.reset_index(drop=True)\n    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n    tmp = tmp.loc[tmp.n<20].drop('n',axis=1)\n    \n    # SAVE PART TO DISK (convert to pandas first uses less memory)\n    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2022-11-10T16:14:44.629032","status":"running"},"tags":[],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-05-23T03:02:45.501622Z","iopub.execute_input":"2023-05-23T03:02:45.502060Z","iopub.status.idle":"2023-05-23T03:06:06.143965Z","shell.execute_reply.started":"2023-05-23T03:02:45.502019Z","shell.execute_reply":"2023-05-23T03:06:06.142780Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n### DISK PART 1\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 2\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 3\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \n\n### DISK PART 4\nProcessing files 0 thru 24 in groups of 5...\n0 , 5 , 10 , 15 , 20 , \nProcessing files 25 thru 49 in groups of 5...\n25 , 30 , 35 , 40 , 45 , \nProcessing files 50 thru 74 in groups of 5...\n50 , 55 , 60 , 65 , 70 , \nProcessing files 75 thru 99 in groups of 5...\n75 , 80 , 85 , 90 , 95 , \nProcessing files 100 thru 124 in groups of 5...\n100 , 105 , 110 , 115 , 120 , \nProcessing files 125 thru 145 in groups of 5...\n125 , 130 , 135 , 140 , 145 , \nCPU times: user 2min 11s, sys: 1min 8s, total: 3min 20s\nWall time: 3min 20s\n","output_type":"stream"}]},{"cell_type":"code","source":"# FREE MEMORY\ndel data_cache, tmp\n_ = gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T03:06:06.148735Z","iopub.execute_input":"2023-05-23T03:06:06.149077Z","iopub.status.idle":"2023-05-23T03:06:06.319114Z","shell.execute_reply.started":"2023-05-23T03:06:06.149046Z","shell.execute_reply":"2023-05-23T03:06:06.318028Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def load_test():    \n    dfs = []\n    for e, chunk_file in enumerate(glob.glob('../input/otto-chunk-data-inparquet-format/test_parquet/*')):\n        chunk = pd.read_parquet(chunk_file)\n        chunk.ts = (chunk.ts/1000).astype('int32')\n        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n        dfs.append(chunk)\n    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n\ntest_df = load_test()\nprint('Test data has shape',test_df.shape)\ntest_df.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T03:06:06.320731Z","iopub.execute_input":"2023-05-23T03:06:06.321485Z","iopub.status.idle":"2023-05-23T03:06:08.074907Z","shell.execute_reply.started":"2023-05-23T03:06:06.321440Z","shell.execute_reply":"2023-05-23T03:06:08.073811Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Test data has shape (6928123, 4)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"    session     aid          ts  type\n0  13099779  245308  1661795832     0\n1  13099779  245308  1661795862     1\n2  13099779  972319  1661795888     0\n3  13099779  972319  1661795898     1\n4  13099779  245308  1661795907     0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>session</th>\n      <th>aid</th>\n      <th>ts</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13099779</td>\n      <td>245308</td>\n      <td>1661795832</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13099779</td>\n      <td>245308</td>\n      <td>1661795862</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13099779</td>\n      <td>972319</td>\n      <td>1661795888</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13099779</td>\n      <td>972319</td>\n      <td>1661795898</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13099779</td>\n      <td>245308</td>\n      <td>1661795907</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\ndef pqt_to_dict(df):\n    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n\n# LOAD THREE CO-VISITATION MATRICES\ntop_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n\nfor k in range(1,DISK_PIECES): \n    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n\n\ntop_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n\nfor k in range(1,DISK_PIECES): \n    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n\ntop_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n\n# TOP CLICKS AND ORDERS IN TEST\n#top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\n#top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n\nprint('Here are size of our 3 co-visitation matrices:')\nprint( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T03:06:08.076885Z","iopub.execute_input":"2023-05-23T03:06:08.077680Z","iopub.status.idle":"2023-05-23T03:08:35.107250Z","shell.execute_reply.started":"2023-05-23T03:06:08.077635Z","shell.execute_reply":"2023-05-23T03:08:35.106162Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Here are size of our 3 co-visitation matrices:\n1837166 1168768 1837166\nCPU times: user 2min 25s, sys: 5 s, total: 2min 30s\nWall time: 2min 27s\n","output_type":"stream"}]},{"cell_type":"code","source":"top_clicks = test_df.loc[test_df['type']== 0,'aid'].value_counts().index.values[:20] \ntop_carts = test_df.loc[test_df['type']== 1,'aid'].value_counts().index.values[:20]\ntop_orders = test_df.loc[test_df['type']== 2,'aid'].value_counts().index.values[:20]","metadata":{"execution":{"iopub.status.busy":"2023-05-23T03:08:35.108873Z","iopub.execute_input":"2023-05-23T03:08:35.110494Z","iopub.status.idle":"2023-05-23T03:08:35.498688Z","shell.execute_reply.started":"2023-05-23T03:08:35.110454Z","shell.execute_reply":"2023-05-23T03:08:35.497531Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#type_weight_multipliers = {'clicks': 1, 'carts': 5, 'orders': 4}\ntype_weight_multipliers = {0: 1, 1: 5, 2: 4}\n\ndef suggest_clicks(df):\n    # USER HISTORY AIDS AND TYPES\n    aids=df.aid.tolist()\n    types = df.type.tolist()\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    # RERANK CANDIDATES USING WEIGHTS\n    if len(unique_aids)>=20:\n        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n        for aid,w,t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n        return sorted_aids\n    # USE \"CLICKS\" CO-VISITATION MATRIX\n    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n    # RERANK CANDIDATES\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n    # USE TOP20 TEST CLICKS\n    return result + list(top_clicks)[:20-len(result)]\n\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T03:08:35.502665Z","iopub.execute_input":"2023-05-23T03:08:35.503111Z","iopub.status.idle":"2023-05-23T03:08:35.547182Z","shell.execute_reply.started":"2023-05-23T03:08:35.503068Z","shell.execute_reply":"2023-05-23T03:08:35.546108Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def suggest_carts(df):\n    # User history aids and types\n    aids = df.aid.tolist()\n    types = df.type.tolist()\n    \n    # UNIQUE AIDS AND UNIQUE BUYS\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    df = df.loc[(df['type'] == 0)|(df['type'] == 1)]\n    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n    \n    # Rerank candidates using weights\n    if len(unique_aids) >= 20:\n        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        \n        # Rerank based on repeat items and types of items\n        for aid,w,t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        \n        # Rerank candidates using\"top_20_carts\" co-visitation matrix\n        aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_buys if aid in top_20_buys]))\n        for aid in aids2: aids_temp[aid] += 0.1\n        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n        return sorted_aids\n    \n    # Use \"cart order\" and \"clicks\" co-visitation matrices\n    aids1 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n    \n    # RERANK CANDIDATES\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids1+aids2).most_common(20) if aid2 not in unique_aids] \n    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n    \n    # USE TOP20 TEST ORDERS\n    return result + list(top_carts)[:20-len(result)]","metadata":{"execution":{"iopub.status.busy":"2023-05-23T03:08:35.548857Z","iopub.execute_input":"2023-05-23T03:08:35.550148Z","iopub.status.idle":"2023-05-23T03:08:35.565498Z","shell.execute_reply.started":"2023-05-23T03:08:35.550110Z","shell.execute_reply":"2023-05-23T03:08:35.564285Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def suggest_buys(df):\n    # USER HISTORY AIDS AND TYPES\n    aids=df.aid.tolist()\n    types = df.type.tolist()\n    # UNIQUE AIDS AND UNIQUE BUYS\n    unique_aids = list(dict.fromkeys(aids[::-1] ))\n    df = df.loc[(df['type']==1)|(df['type']==2)]\n    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n    # RERANK CANDIDATES USING WEIGHTS\n    if len(unique_aids)>=20:\n        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n        aids_temp = Counter() \n        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n        for aid,w,t in zip(aids,weights,types): \n            aids_temp[aid] += w * type_weight_multipliers[t]\n        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n        for aid in aids3: aids_temp[aid] += 0.1\n        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n        return sorted_aids\n    # USE \"CART ORDER\" CO-VISITATION MATRIX\n    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n    # RERANK CANDIDATES\n    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n    # USE TOP20 TEST ORDERS\n    return result + list(top_orders)[:20-len(result)]","metadata":{"execution":{"iopub.status.busy":"2023-05-23T03:08:35.567215Z","iopub.execute_input":"2023-05-23T03:08:35.567630Z","iopub.status.idle":"2023-05-23T03:08:35.587806Z","shell.execute_reply.started":"2023-05-23T03:08:35.567591Z","shell.execute_reply":"2023-05-23T03:08:35.586747Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%%time\n\npred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_clicks(x)\n)\n\npred_df_carts = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_carts(x)\n)\n\npred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_buys(x)\n)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2023-05-23T03:08:35.589332Z","iopub.execute_input":"2023-05-23T03:08:35.590329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\norders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\ncarts_pred_df = pd.DataFrame(pred_df_carts.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\npred_df.columns = [\"session_type\", \"labels\"]\npred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\npred_df.to_csv(\"submission.csv\", index=False)\npred_df.head()","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}